{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93cdc8a",
   "metadata": {},
   "source": [
    "# UL Hard Constraint Physics Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5e905",
   "metadata": {},
   "source": [
    "## Jacobian Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641597bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jacobi_layer(torch.nn.Module):\n",
    "    def __init__(self, nx=21, length=0.1, bcs=None):\n",
    "        super().__init__()\n",
    "        self.length = length\n",
    "        self.bcs = bcs\n",
    "        # The weight 1/4(u_(i, j-1), u_(i, j+1), u_(i-1, j), u_(i+1, j))\n",
    "        self.weight = torch.Tensor([[[[0, 0.25, 0], [0.25, 0, 0.25], [0, 0.25, 0]]]])\n",
    "        # Mesh\n",
    "        self.nx = nx\n",
    "        self.scale_factor = 1  # self.nx/200\n",
    "        TEMPER_COEFFICIENT = 1  # 50\n",
    "        STRIDE = self.length / (self.nx - 1)\n",
    "        # ((l/(nx))^2)/(4*cof)*m*input(x, y)\n",
    "        self.cof = 0.25 * STRIDE ** 2 / TEMPER_COEFFICIENT\n",
    "\n",
    "    def jacobi(self, x):\n",
    "        return conv2d(x, self.weight.to(device=x.device), bias=None, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, layout, heat, n_iter):\n",
    "        # Source item\n",
    "        f = self.cof * layout\n",
    "        # The nodes which are not in boundary\n",
    "        G = torch.ones_like(heat).detach()\n",
    "\n",
    "        if self.bcs is None or len(self.bcs) == 0 or len(self.bcs[0]) == 0:  # all are Dirichlet bcs\n",
    "            pass\n",
    "        else:\n",
    "            for bc in self.bcs:\n",
    "                if bc[0][1] == 0 and bc[1][1] == 0:\n",
    "                    idx_start = round(bc[0][0] * self.nx / self.length)\n",
    "                    idx_end = round(bc[1][0] * self.nx / self.length)\n",
    "                    G[..., idx_start:idx_end, :1] = torch.zeros_like(G[..., idx_start:idx_end, :1])\n",
    "                elif bc[0][1] == self.length and bc[1][1] == self.length:\n",
    "                    idx_start = round(bc[0][0] * self.nx / self.length)\n",
    "                    idx_end = round(bc[1][0] * self.nx / self.length)\n",
    "                    G[..., idx_start:idx_end, -1:] = torch.zeros_like(G[..., idx_start:idx_end, -1:])\n",
    "                elif bc[0][0] == 0 and bc[1][0] == 0:\n",
    "                    idx_start = round(bc[0][1] * self.nx / self.length)\n",
    "                    idx_end = round(bc[1][1] * self.nx / self.length)\n",
    "                    G[..., :1, idx_start:idx_end] = torch.zeros_like(G[..., :1, idx_start:idx_end])\n",
    "                elif bc[0][0] == self.length and bc[1][0] == self.length:\n",
    "                    idx_start = round(bc[0][1] * self.nx / self.length)\n",
    "                    idx_end = round(bc[1][1] * self.nx / self.length)\n",
    "                    G[..., -1:, idx_start:idx_end] = torch.zeros_like(G[..., -1:, idx_start:idx_end])\n",
    "                else:\n",
    "                    raise ValueError(\"bc error!\")\n",
    "        for i in range(n_iter):\n",
    "            if i == 0:\n",
    "                x = F.pad(heat * G, [1, 1, 1, 1], mode='reflect')\n",
    "            else:\n",
    "                x = F.pad(x, [1, 1, 1, 1], mode='reflect')\n",
    "            x = G * (self.jacobi(x) + f)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067921a6",
   "metadata": {},
   "source": [
    "## Online Hard Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHEMF12d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Weighted Loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_fun, weight=None):\n",
    "        super(OHEMF12d, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.loss_fun = loss_fun\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        diff = self.loss_fun(inputs, targets, reduction='none').detach()\n",
    "        min, max = torch.min(diff.view(diff.shape[0], -1), dim=1)[0], torch.max(diff.view(diff.shape[0], -1), dim=1)[0]\n",
    "        if inputs.ndim == 4:\n",
    "            min, max = min.reshape(diff.shape[0], 1, 1, 1).expand(diff.shape), \\\n",
    "                       max.reshape(diff.shape[0], 1, 1, 1).expand(diff.shape)\n",
    "        elif inputs.ndim == 3:\n",
    "            min, max = min.reshape(diff.shape[0], 1, 1).expand(diff.shape), \\\n",
    "                       max.reshape(diff.shape[0], 1, 1).expand(diff.shape)\n",
    "        diff = 10.0 * (diff - min) / (max - min)\n",
    "        return torch.mean(torch.abs(diff * (inputs - targets)))\n",
    "    \n",
    "    # loss function with OHEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263337a6",
   "metadata": {},
   "source": [
    "### In UL Model with Hard Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, batch_idx):\n",
    "        layout, _ = batch\n",
    "        heat_pre = self(layout)\n",
    "\n",
    "        layout = layout * self.hparams.std_layout + self.hparams.mean_layout\n",
    "        # The loss of govern equation + Online Hard Sample Mining\n",
    "        with torch.no_grad():\n",
    "            heat_jacobi = self.jacobi(layout, heat_pre, 1)\n",
    "\n",
    "        loss_fun = OHEMF12d(loss_fun=F.l1_loss)\n",
    "        \n",
    "        loss_jacobi = loss_fun(heat_pre - heat_jacobi, torch.zeros_like(heat_pre - heat_jacobi))\n",
    "\n",
    "        loss = loss_jacobi\n",
    "        # Physics loss\n",
    "\n",
    "        self.log('loss_jacobi', loss_jacobi)\n",
    "        self.log('loss', loss)\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6181c7c",
   "metadata": {},
   "source": [
    "# UL Soft Constraint Physics Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5524e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jacobi_layerSoft(torch.nn.Module): #We used hard constraints, so we didnt use this function but it is good to have for different testing scenarios. \n",
    "    def __init__(\n",
    "            self, nx=21, length=0.1, bcs=None\n",
    "    ):\n",
    "        super(Jacobi_layerSoft, self).__init__()\n",
    "        self.length = length\n",
    "        self.bcs = bcs\n",
    "        # The weight 1/4(u_(i, j-1), u_(i, j+1), u_(i-1, j), u_(i+1, j))\n",
    "        self.weight = torch.Tensor([[[[0, 0.25, 0], [0.25, 0, 0.25], [0, 0.25, 0]]]])\n",
    "        # Mesh\n",
    "        self.nx = nx\n",
    "        self.scale_factor = 1  # self.nx/200\n",
    "        TEMPER_COEFFICIENT = 1  # 50\n",
    "        STRIDE = self.length / (self.nx - 1)\n",
    "        # ((l/(nx))^2)/(4*cof)*m*input(x, y)\n",
    "        self.cof = 0.25 * STRIDE ** 2 / TEMPER_COEFFICIENT\n",
    "\n",
    "    def jacobi(self, x):\n",
    "        return conv2d(x, self.weight.to(device=x.device), bias=None, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, layout, heat, n_iter):\n",
    "        # Source item\n",
    "        f = self.cof * layout\n",
    "        # The nodes which are not in boundary\n",
    "        G = torch.ones_like(heat).detach()\n",
    "\n",
    "        if self.bcs is None or len(self.bcs) == 0 or len(self.bcs[0]) == 0:  # all are Dirichlet bcs\n",
    "            pass\n",
    "        else:\n",
    "            for bc in self.bcs:\n",
    "                if bc[0][1] == 0 and bc[1][1] == 0:\n",
    "                    idx_start = round(bc[0][0] * self.nx / self.length)\n",
    "                    idx_end = round(bc[1][0] * self.nx / self.length)\n",
    "                    G[..., idx_start:idx_end, :1] = torch.zeros_like(G[..., idx_start:idx_end, :1])\n",
    "                elif bc[0][1] == self.length and bc[1][1] == self.length:\n",
    "                    idx_start = round(bc[0][0] * self.nx / self.length)\n",
    "                    idx_end = round(bc[1][0] * self.nx / self.length)\n",
    "                    G[..., idx_start:idx_end, -1:] = torch.zeros_like(G[..., idx_start:idx_end, -1:])\n",
    "                elif bc[0][0] == 0 and bc[1][0] == 0:\n",
    "                    idx_start = round(bc[0][1] * self.nx / self.length)\n",
    "                    idx_end = round(bc[1][1] * self.nx / self.length)\n",
    "                    G[..., :1, idx_start:idx_end] = torch.zeros_like(G[..., :1, idx_start:idx_end])\n",
    "                elif bc[0][0] == self.length and bc[1][0] == self.length:\n",
    "                    idx_start = round(bc[0][1] * self.nx / self.length)\n",
    "                    idx_end = round(bc[1][1] * self.nx / self.length)\n",
    "                    G[..., -1:, idx_start:idx_end] = torch.zeros_like(G[..., -1:, idx_start:idx_end])\n",
    "                else:\n",
    "                    raise ValueError(\"bc error!\")\n",
    "        for i in range(n_iter):\n",
    "            if i == 0:\n",
    "                x = F.pad(heat, [1, 1, 1, 1], mode='reflect') #Not included contribution from G\n",
    "            else:\n",
    "                x = F.pad(x, [1, 1, 1, 1], mode='reflect')\n",
    "            x = G * (self.jacobi(x) + f)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ef240",
   "metadata": {},
   "source": [
    "### In UL Model with Soft Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training_step(self, batch, batch_idx):\n",
    "        layout, _ = batch\n",
    "        heat_pre = self(layout) \n",
    "\n",
    "        layout = layout * self.hparams.std_layout + self.hparams.mean_layout\n",
    "\n",
    "        with torch.no_grad():\n",
    "            heat_jacobi = self.loss(layout, heat_pre, 1)\n",
    "\n",
    "        loss_fun = LDLU.OHEMF12d(loss_fun=F.l1_loss)\n",
    "        \n",
    "        loss_jacobi = loss_fun(heat_pre - heat_jacobi, torch.zeros_like(heat_pre - heat_jacobi))\n",
    "        loss_D = F.l1_loss(heat_pre[..., 90:110, :1], torch.zeros_like(heat_pre[..., 90:110, :1]))\n",
    "\n",
    "        loss = loss_jacobi + 0.001*loss_D\n",
    "        # Physics loss + data loss\n",
    "\n",
    "        self.log('loss_jacobi', loss_jacobi)\n",
    "        self.log('loss_D', loss_D)\n",
    "        self.log('loss', loss)\n",
    "\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26668c1",
   "metadata": {},
   "source": [
    "## SL Model (Data Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training_step(self, batch, batch_idx):\n",
    "        layout, heat = batch\n",
    "        heat_pre = self(layout)\n",
    "        \n",
    "        loss_fun = torch.nn.L1Loss()\n",
    "        loss = loss_fun(heat_pre, heat - 298.0)\n",
    "        # Data loss\n",
    "        \n",
    "        self.log('loss',loss)\n",
    "        \n",
    "        return {\"loss\": loss}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
